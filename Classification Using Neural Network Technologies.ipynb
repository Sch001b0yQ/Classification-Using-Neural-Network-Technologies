{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af97e5c3-7295-40b4-ad15-91d847df3da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install --upgrade pip\n",
    "\n",
    "\n",
    "%pip uninstall -y torch torchvision torchaudio\n",
    "\n",
    "\n",
    "%pip install --no-cache-dir torch torchvision torchaudio \\\n",
    "               --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "%pip install timm albumentations==1.4.18 opencv-python-headless tqdm resnest ipywidgets\n",
    "\n",
    "\n",
    "!jupyter nbextension enable --py widgetsnbextension --sys-prefix\n",
    "!jupyter labextension install @jupyter-widgets/jupyterlab-manager --no-build\n",
    "!jupyter lab build  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ebd301-fa4e-41e6-95c1-ce267087fee8",
   "metadata": {},
   "source": [
    "Vision Transformer 61 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fd33a3-b119-49ad-a004-41ccf67bff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, pandas as pd\n",
    "\n",
    "BASE_DIR = r\"F:\\GroceryStoreDataset-master\"             \n",
    "TRAIN_TXT = os.path.join(BASE_DIR, \"dataset\", \"train.txt\")\n",
    "VAL_TXT   = os.path.join(BASE_DIR, \"dataset\", \"val.txt\")\n",
    "\n",
    "COLS = [\"rel_path\", \"fine\", \"coarse\"]\n",
    "train_df = pd.read_csv(TRAIN_TXT, sep=r\"\\s+\", header=None, names=COLS, dtype=str)\n",
    "val_df   = pd.read_csv(VAL_TXT,   sep=r\"\\s+\", header=None, names=COLS, dtype=str)\n",
    "\n",
    "def make_abs(p: str) -> str:\n",
    "    p = p.rstrip(\",; \")                 \n",
    "    if not p.startswith(\"dataset\"):     \n",
    "        p = os.path.join(\"dataset\", p)\n",
    "    return os.path.join(BASE_DIR, p)\n",
    "\n",
    "train_df[\"image_path\"] = train_df[\"rel_path\"].apply(make_abs)\n",
    "val_df[\"image_path\"]   = val_df[\"rel_path\"].apply(make_abs)\n",
    "\n",
    "train_df[\"label\"] = train_df[\"fine\"].str.replace(r\"[^\\d]\", \"\", regex=True).astype(int)\n",
    "val_df[\"label\"]   = val_df[\"fine\"].str.replace(r\"[^\\d]\", \"\", regex=True).astype(int)\n",
    "\n",
    "train_df = train_df[[\"image_path\", \"label\"]]\n",
    "val_df   = val_df[[\"image_path\", \"label\"]]\n",
    "\n",
    "train_df = train_df[train_df[\"image_path\"].apply(os.path.exists)].reset_index(drop=True)\n",
    "val_df   = val_df[val_df[\"image_path\"].apply(os.path.exists)].reset_index(drop=True)\n",
    "\n",
    "print(\"Train / Val samples:\", len(train_df), \"/\", len(val_df))\n",
    "print(train_df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab6c32d-879d-4e0a-bb10-b9e5bf99e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8,1.0)),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.paths  = df['image_path'].tolist()\n",
    "        self.labels = df['label'].tolist()\n",
    "        self.tf     = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img = Image.open(self.paths[i]).convert('RGB')\n",
    "        if self.tf:\n",
    "            img = self.tf(img)\n",
    "        return img, self.labels[i]\n",
    "\n",
    "print(\"Transforms & Dataset ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b336f61-d18f-4ede-98b4-04e7b528ce20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 16    \n",
    "\n",
    "train_ds = CustomDataset(train_df, transform=train_transform)\n",
    "val_ds   = CustomDataset(val_df,   transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=0, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=0, pin_memory=True)\n",
    "\n",
    "print(f\"Train: {len(train_ds)} samples, Val: {len(val_ds)} samples, Batch={BATCH_SIZE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3fad38-0156-4034-b3fe-d15468283dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "model = timm.create_model('vit_base_patch16_224',\n",
    "                         pretrained=True,\n",
    "                         num_classes=NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "for p in model.parameters():     p.requires_grad = False\n",
    "for p in model.head.parameters(): p.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=LR_HEAD,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=TOTAL_EPOCHS)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(1, TOTAL_EPOCHS+1):\n",
    "    print(f\"\\n=== Epoch {epoch}/{TOTAL_EPOCHS} ===\")\n",
    "    if epoch == HEAD_ONLY_EPOCHS + 1:\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = True\n",
    "        optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                                      lr=LR_FULL,\n",
    "                                      weight_decay=WEIGHT_DECAY)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=FULL_EPOCHS)\n",
    "        print(f\" Unfroze full backbone, LR set to {LR_FULL}\")\n",
    "\n",
    "    model.train()\n",
    "    tr_loss = 0.0\n",
    "    tr_correct = 0\n",
    "    train_bar = tqdm(train_loader, desc=\"Train\", leave=False)\n",
    "    for step, (imgs, lbls) in enumerate(train_bar, start=1):\n",
    "        imgs, lbls = imgs.to(DEVICE), lbls.to(DEVICE)\n",
    "\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, lbls)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_size = imgs.size(0)\n",
    "        tr_loss    += loss.item() * batch_size\n",
    "        tr_correct += (logits.argmax(1) == lbls).sum().item()\n",
    "\n",
    "        num_seen = step * batch_size\n",
    "        train_bar.set_postfix({\n",
    "            \"loss\": f\"{tr_loss/num_seen:.3f}\",\n",
    "            \"acc\":  f\"{100*tr_correct/num_seen:.1f}%\"\n",
    "        })\n",
    "\n",
    "    train_acc  = 100 * tr_correct / len(train_loader.dataset)\n",
    "    train_loss = tr_loss / len(train_loader.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in tqdm(val_loader, desc=\"Val  \", leave=False):\n",
    "            imgs, lbls = imgs.to(DEVICE), lbls.to(DEVICE)\n",
    "            out = model(imgs)\n",
    "            val_loss    += criterion(out, lbls).item() * imgs.size(0)\n",
    "            val_correct += (out.argmax(1) == lbls).sum().item()\n",
    "\n",
    "    val_acc  = 100 * val_correct / len(val_loader.dataset)\n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_vit.pth\")\n",
    "        print(f\"New best! val_acc = {val_acc:.2f}%\")\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d}/{TOTAL_EPOCHS} | \"\n",
    "        f\"train_loss={train_loss:.3f}, train_acc={train_acc:.1f}% | \"\n",
    "        f\"val_loss={val_loss:.3f}, val_acc={val_acc:.1f}%\"\n",
    "    )\n",
    "\n",
    "torch.save(model.state_dict(), \"last_vit.pth\")\n",
    "print(\"Training complete – last_vit.pth saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d70c6a-caf6-4cc2-b2f8-c60c3d8a71b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for imgs, lbls in val_loader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        out = model(imgs)\n",
    "        preds = out.argmax(1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(lbls.numpy())\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(cm, annot=False, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.xlabel(\"Pred\")\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(all_labels, all_preds, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16be7336-eb6d-41d5-bd48-9610ba6172b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8e1119-8549-4ff7-9ee3-93d5bb86be7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, pandas as pd\n",
    "\n",
    "with open(\"training.log\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "pattern = re.compile(\n",
    "    r\"Epoch\\s+(\\d+)[/\\\\]\\d+\\s*\\|\\s*\"\n",
    "    r\"train_loss=([\\d.]+),\\s*train_acc=([\\d.]+)%\\s*\\|\\s*\"\n",
    "    r\"val_loss=([\\d.]+),\\s*val_acc=([\\d.]+)%\"\n",
    ")\n",
    "\n",
    "epochs, tr_loss, tr_acc, vl_loss, vl_acc = [], [], [], [], []\n",
    "for line in lines:\n",
    "    m = pattern.search(line)\n",
    "    if m:\n",
    "        epochs.append(int(m.group(1)))\n",
    "        tr_loss.append(float(m.group(2)))\n",
    "        tr_acc.append(float(m.group(3)))\n",
    "        vl_loss.append(float(m.group(4)))\n",
    "        vl_acc.append(float(m.group(5)))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"epoch\":      epochs,\n",
    "    \"train_loss\": tr_loss,\n",
    "    \"train_acc\":  tr_acc,\n",
    "    \"val_loss\":   vl_loss,\n",
    "    \"val_acc\":    vl_acc\n",
    "})\n",
    "df.to_csv(\"parsed_metrics.csv\", index=False)\n",
    "print(\"Saved parsed_metrics.csv with\", len(df), \"rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1b3f82-f726-4110-8d91-86892de33d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"parsed_metrics.csv\")\n",
    "print(df.head())\n",
    "print(df.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a3bf40-99c5-413d-bca6-5279e9c05d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"parsed_metrics.csv\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(df[\"epoch\"], df[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(df[\"epoch\"], df[\"val_loss\"],   label=\"val_loss\")\n",
    "plt.xlabel(\"Епоха\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Train vs Val Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(df[\"epoch\"], df[\"train_acc\"], label=\"train_acc\")\n",
    "plt.plot(df[\"epoch\"], df[\"val_acc\"],   label=\"val_acc\")\n",
    "plt.xlabel(\"Епоха\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Train vs Val Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "83f507da-3625-433f-9ddb-2e8d3caae114",
   "metadata": {},
   "source": [
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "torch.save(model.state_dict(), \"baseline_vit.pth\")\n",
    "\n",
    "baseline_config = {\n",
    "    \"arch\":            \"vit_base_patch16_224\",\n",
    "    \"pretrained\":      True,\n",
    "    \"img_size\":        128,\n",
    "    \"batch_size\":      BATCH_SIZE,\n",
    "    \"head_only_epochs\": HEAD_ONLY_EPOCHS,\n",
    "    \"full_epochs\":     FULL_EPOCHS,\n",
    "    \"lr_head\":         LR_HEAD,\n",
    "    \"lr_full\":         LR_FULL,\n",
    "    \"weight_decay\":    WEIGHT_DECAY,\n",
    "    \"transforms\": {\n",
    "        \"train\": repr(train_transform),\n",
    "        \"val\":   repr(val_transform)\n",
    "    }\n",
    "}\n",
    "with open(\"baseline_vit_config.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(baseline_config, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "df = pd.read_csv(\"parsed_metrics.csv\")  \n",
    "df.to_csv(\"baseline_vit_log.csv\", index=False)\n",
    "\n",
    "print(\"Збережено:\")\n",
    "print(\" • baseline_vit.pth           – ваги моделі\")\n",
    "print(\" • baseline_vit_config.json   – конфігурація\")\n",
    "print(\" • baseline_vit_log.csv       – train/val метрики по епохах\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5ae7b5-942e-42b0-83eb-1666d8dc4f72",
   "metadata": {},
   "source": [
    "ConvNext 61 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648d0054-0ddb-430e-9296-cfaea395acbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR  = r\"F:\\GroceryStoreDataset-master\"\n",
    "TRAIN_TXT = os.path.join(BASE_DIR, \"dataset\", \"train.txt\")\n",
    "VAL_TXT   = os.path.join(BASE_DIR, \"dataset\", \"val.txt\")\n",
    "\n",
    "COLS = [\"rel_path\", \"fine\", \"coarse\"]\n",
    "train_df = pd.read_csv(TRAIN_TXT, sep=r\"\\s+\", header=None, names=COLS, dtype=str)\n",
    "val_df   = pd.read_csv(VAL_TXT,   sep=r\"\\s+\", header=None, names=COLS, dtype=str)\n",
    "\n",
    "def make_abs(p: str) -> str:\n",
    "    p = p.rstrip(\", \")\n",
    "    if not p.startswith(\"dataset\"):\n",
    "        p = os.path.join(\"dataset\", p)\n",
    "    return os.path.join(BASE_DIR, p)\n",
    "\n",
    "train_df[\"image_path\"] = train_df[\"rel_path\"].apply(make_abs)\n",
    "val_df[\"image_path\"]   = val_df[\"rel_path\"].apply(make_abs)\n",
    "\n",
    "train_df[\"label\"] = train_df[\"fine\"].str.replace(r\"[^\\d]\", \"\", regex=True).astype(int)\n",
    "val_df[\"label\"]   = val_df[\"fine\"].str.replace(r\"[^\\d]\", \"\", regex=True).astype(int)\n",
    "\n",
    "train_df = train_df[[\"image_path\", \"label\"]]\n",
    "val_df   = val_df[[\"image_path\", \"label\"]]\n",
    "\n",
    "train_df = train_df[train_df[\"image_path\"].apply(os.path.exists)].reset_index(drop=True)\n",
    "val_df   = val_df[val_df[\"image_path\"].apply(os.path.exists)].reset_index(drop=True)\n",
    "\n",
    "NUM_CLASSES = train_df[\"label\"].nunique()\n",
    "\n",
    "print(f\"Train samples: {len(train_df)}, Val samples: {len(val_df)}, Classes = {NUM_CLASSES}\")\n",
    "print(train_df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bca774-df7b-430a-b815-f0c719be5cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8,1.0)),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.ColorJitter(0.2,0.2,0.2,0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "class GroceryDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.paths  = df[\"image_path\"].tolist()\n",
    "        self.labels = df[\"label\"].tolist()\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, self.labels[idx]\n",
    "\n",
    "BATCH_SIZE = 16  \n",
    "train_ds = GroceryDataset(train_df, transform=train_transform)\n",
    "val_ds   = GroceryDataset(val_df,   transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "\n",
    "print(f\"DataLoaders ready: train batches = {len(train_loader)}, val batches = {len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6314534-bd52-45d1-8d63-2118a1584a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import timm\n",
    "\n",
    "print(\"NUM_CLASSES =\", NUM_CLASSES)\n",
    "\n",
    "DEVICE           = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "head_only_epochs = 10\n",
    "full_epochs      = 50\n",
    "TOTAL_EPOCHS     = head_only_epochs + full_epochs\n",
    "lr_head          = 3e-4\n",
    "lr_full          = 1e-5\n",
    "weight_decay     = 1e-4\n",
    "\n",
    "model = timm.create_model(\"convnext_base\", pretrained=True, num_classes=0)\n",
    "\n",
    "in_ch = model.head.norm.normalized_shape[0] \n",
    "model.head = nn.Sequential(\n",
    "    model.head.global_pool,    \n",
    "    nn.Flatten(1),            \n",
    "    nn.LayerNorm(in_ch),       \n",
    "    nn.Dropout(0.0),           \n",
    "    nn.Linear(in_ch, NUM_CLASSES)  \n",
    ")\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in model.head.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=lr_head, weight_decay=weight_decay\n",
    ")\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=head_only_epochs)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scaler    = torch.cuda.amp.GradScaler()\n",
    "\n",
    "best_val_acc   = 0.0\n",
    "train_losses, train_accs = [], []\n",
    "val_losses,   val_accs   = [], []\n",
    "\n",
    "total     = sum(p.numel() for p in model.parameters())\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\" Model ready on {DEVICE}\")\n",
    "print(f\"Trainable params: {trainable}/{total} ({100*trainable/total:.2f}%)\")\n",
    "print(f\"Запускаємо тренування на {TOTAL_EPOCHS} епох \"\n",
    "      f\"({head_only_epochs} head-only + {full_epochs} full-fine)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aeb9c1-1aa6-4c1a-9a5f-64e2fb3fd1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, TOTAL_EPOCHS+1):\n",
    "    if epoch == head_only_epochs + 1:\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = True\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr_full, weight_decay=weight_decay)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=full_epochs)\n",
    "        print(f\"Розморожено всі шари — lr ← {lr_full}\")\n",
    "\n",
    "    model.train()\n",
    "    running_loss, running_correct = 0.0, 0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            logits = model(imgs)\n",
    "            loss   = criterion(logits, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss    += loss.item() * imgs.size(0)\n",
    "        running_correct += (logits.argmax(1) == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_acc  = 100 * running_correct / len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss, val_correct = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                out  = model(imgs)\n",
    "                loss = criterion(out, labels)\n",
    "            val_loss    += loss.item() * imgs.size(0)\n",
    "            val_correct += (out.argmax(1) == labels).sum().item()\n",
    "\n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "    val_acc  = 100 * val_correct / len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_convnext.pth\")\n",
    "        print(f\"Епоха {epoch}: new best val_acc = {val_acc:.2f}%\")\n",
    "\n",
    "    print(f\"Епоха {epoch:02d}/{TOTAL_EPOCHS} | \"\n",
    "          f\"train_loss={train_loss:.3f}, train_acc={train_acc:.1f}% | \"\n",
    "          f\"val_loss={val_loss:.3f}, val_acc={val_acc:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d7ec51-7fcf-4271-965f-a4aa8e7efa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "torch.save(model.state_dict(), \"last_convnext.pth\")\n",
    "print(\"Фінальна модель збережена як last_convnext.pth\")\n",
    "\n",
    "logs_df = pd.DataFrame({\n",
    "    \"epoch\":      list(range(1, TOTAL_EPOCHS+1)),\n",
    "    \"train_loss\": train_losses,\n",
    "    \"train_acc\":  train_accs,\n",
    "    \"val_loss\":   val_losses,\n",
    "    \"val_acc\":    val_accs\n",
    "})\n",
    "logs_df.to_csv(\"convnext_training_log.csv\", index=False)\n",
    "print(\"Лог тренування збережено в convnext_training_log.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6598ac-d878-40fd-96ca-595e61278e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logs_df = pd.read_csv(\"convnext_training_log.csv\")\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(logs_df[\"epoch\"], logs_df[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(logs_df[\"epoch\"], logs_df[\"val_loss\"],   label=\"val_loss\")\n",
    "plt.xlabel(\"Епоха\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Train vs Val Loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(logs_df[\"epoch\"], logs_df[\"train_acc\"], label=\"train_acc\")\n",
    "plt.plot(logs_df[\"epoch\"], logs_df[\"val_acc\"],   label=\"val_acc\")\n",
    "plt.xlabel(\"Епоха\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Train vs Val Accuracy\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4297e4-e273-4086-8a9b-89de2ea375b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(\"best_convnext.pth\"))\n",
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "        out = model(imgs)\n",
    "        preds = out.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(cm, cmap=\"Blues\", cbar=True, square=True)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix — ConvNeXt-Base на валідації\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"confusion_matrix_convnext.png\")\n",
    "plt.show()\n",
    "\n",
    "report_dict = classification_report(all_labels, all_preds, output_dict=True, zero_division=0)\n",
    "report_df = pd.DataFrame(report_dict).T\n",
    "report_df.to_csv(\"classification_report_convnext.csv\", index=True)\n",
    "\n",
    "print(\"Збережено:\")\n",
    "print(\"   • confusion_matrix_convnext.png\")\n",
    "print(\"   • classification_report_convnext.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a14300-1041-41a2-88fe-c2961d2c9696",
   "metadata": {},
   "source": [
    "resnet152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aec3e79-bed3-4bc5-8671-189f4aea1d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms.v2 import MixUp, RandomErasing\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfe7412-9658-4d3b-b4af-5f311f84d8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandAugment(num_ops=2, magnitude=9),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.5),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f336f0-121f-4579-9320-ce5e54dc1a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4,\n",
    "                           saturation=0.3, hue=0.04),          \n",
    "    transforms.RandomAffine(degrees=5, translate=(0.05, 0.05)),\n",
    "    transforms.RandAugment(num_ops=2, magnitude=9),\n",
    "    \n",
    "    transforms.ToTensor(),                                     \n",
    "    transforms.RandomErasing(p=0.25),                          \n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652cd949-4855-47e7-854c-beca4fc4d2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ImageFolder(\"F:/GroceryStoreDataset-master/dataset/train\", transform=train_transform)\n",
    "val_ds   = ImageFolder(\"F:/GroceryStoreDataset-master/dataset/val\", transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "NUM_CLASSES = len(train_ds.classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e99fc4-9761-43bc-8591-4e1ce09a265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet152(pretrained=True)\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features, NUM_CLASSES)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd0b27d-9bc5-4f32-9d25-1386558260ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "mixup = MixUp(alpha=1.0, num_classes=NUM_CLASSES)\n",
    "\n",
    "HEAD_ONLY_EPOCHS = 10\n",
    "TOTAL_EPOCHS = 60\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"fc\" not in name:\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87a254f-b13c-47de-a596-97c0192f67b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "best_val_acc = 0.0\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(TOTAL_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{TOTAL_EPOCHS}\", leave=False)\n",
    "    for images, labels in loop:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        if random.random() < 0.6:\n",
    "            images, labels = mixup(images, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        if labels.ndim == 2:  \n",
    "            labels = labels.argmax(dim=1)\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        running_corrects += torch.sum(preds == labels).item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        loop.set_postfix(loss=running_loss/total, acc=running_corrects/total*100)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    epoch_train_loss = running_loss / total\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_correct += torch.sum(preds == labels).item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_acc = val_correct / val_total\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_resnet152.pth\")\n",
    "\n",
    "    print(f\"Епоха {epoch+1:02d} | val_acc={val_acc*100:.2f}%, best={best_val_acc*100:.2f}%\")\n",
    "\n",
    "    if epoch + 1 == HEAD_ONLY_EPOCHS:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(TOTAL_EPOCHS - HEAD_ONLY_EPOCHS))\n",
    "        print(\"==> FULL-FINETUNE PHASE\")\n",
    "\n",
    "log_df = pd.DataFrame({\n",
    "    \"epoch\": list(range(1, TOTAL_EPOCHS+1)),\n",
    "    \"train_loss\": train_losses,\n",
    "    \"val_acc\": [x*100 for x in val_accuracies]  \n",
    "})\n",
    "csv_name = f\"training_log_resnet152_{datetime.now():%Y%m%d_%H%M%S}.csv\"\n",
    "log_df.to_csv(csv_name, index=False)\n",
    "print(f\"Лог тренування збережено у \\\"{csv_name}\\\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d43986-5f23-4669-a681-38b9c7ba3e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log_df = pd.read_csv(\"training_log_resnet152_20250529_204320.csv\")\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(log_df[\"epoch\"], log_df[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.title(\"Train Loss по епохах\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(log_df[\"epoch\"], log_df[\"val_acc\"], label=\"Val Accuracy\", color=\"orange\")\n",
    "plt.title(\"Validation Accuracy (%) по епохах\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6cf7f2-86de-4d23-bbd7-f5ece1035efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = torchvision.models.resnet152(\n",
    "    weights=torchvision.models.ResNet152_Weights.IMAGENET1K_V2\n",
    ")\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features, NUM_CLASSES)\n",
    "model = model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"best_resnet152.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(all_labels, all_preds, target_names=train_ds.classes))\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=train_ds.classes)\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "disp.plot(ax=ax, xticks_rotation=90)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e241379-d2a6-4e74-b19b-8300939b0284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "\n",
    "mixed_train_ds = ConcatDataset([train_ds, test_ds])   \n",
    "mixed_loader   = DataLoader(mixed_train_ds, batch_size=32,\n",
    "                            shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "for p in model.parameters():        p.requires_grad = False\n",
    "for p in model.fc.parameters():     p.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.fc.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "criterion  = torch.nn.CrossEntropyLoss()\n",
    "EPOCHS_FINE = 5\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "for ep in range(EPOCHS_FINE):\n",
    "    model.train(); running=0; correct=0; total=0\n",
    "    loop = tqdm(mixed_loader, desc=f\"Fine {ep+1}/{EPOCHS_FINE}\", leave=False)\n",
    "    for imgs, lbls in loop:\n",
    "        imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = model(imgs); loss = criterion(out, lbls)\n",
    "        loss.backward(); optimizer.step()\n",
    "        running += loss.item()*lbls.size(0)\n",
    "        correct += (out.argmax(1)==lbls).sum().item(); total += lbls.size(0)\n",
    "        loop.set_postfix(loss=running/total, acc=correct/total*100)\n",
    "    print(f\"Fine-epoch {ep+1}:  train_acc {correct/total*100:.1f}%\")\n",
    "\n",
    "torch.save(model.state_dict(), \"resnet152_finetuned.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6af7899-1fc9-40be-a90a-eb3bc0cc0176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "for name, p in model.named_parameters():\n",
    "    if name.startswith(\"layer4\") or name.startswith(\"fc\"):\n",
    "        p.requires_grad = True\n",
    "    else:\n",
    "        p.requires_grad = False\n",
    "\n",
    "opt = torch.optim.AdamW([\n",
    "    {\"params\": [p for n, p in model.named_parameters() if n.startswith(\"layer4\")],\n",
    "     \"lr\": 1e-5},\n",
    "    {\"params\": model.fc.parameters(), \"lr\": 1e-4}\n",
    "], weight_decay=1e-4)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "EPOCHS_FINE = 4\n",
    "\n",
    "for ep in range(EPOCHS_FINE):\n",
    "    model.train(); running=0; correct=0; total=0\n",
    "    loop = tqdm(mixed_loader, desc=f\"Fine {ep+1}/{EPOCHS_FINE}\", leave=False)\n",
    "    for imgs, lbls in loop:\n",
    "        imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "        opt.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = model(imgs)\n",
    "            loss = criterion(out, lbls)\n",
    "        loss.backward(); opt.step()\n",
    "\n",
    "        running += loss.item() * lbls.size(0)\n",
    "        correct += (out.argmax(1) == lbls).sum().item()\n",
    "        total   += lbls.size(0)\n",
    "        loop.set_postfix(loss=running/total, acc=f\"{correct/total*100:.1f}%\")\n",
    "    print(f\"Fine-epoch {ep+1}:  train_acc {correct/total*100:.1f}%\")\n",
    "\n",
    "torch.save(model.state_dict(), \"resnet152_layer4_finetuned.pth\")\n",
    "print(\"Збережено: resnet152_layer4_finetuned.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb16b4a1-b572-4a8c-9237-f116658c80f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "test_transform = val_transform   \n",
    "\n",
    "test_root = r\"F:/real_test\"    \n",
    "test_ds   = ImageFolder(test_root, transform=test_transform)\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Test samples: {len(test_ds)},  Classes: {test_ds.classes}\")\n",
    "\n",
    "model = torchvision.models.resnet152(\n",
    "    weights=torchvision.models.ResNet152_Weights.IMAGENET1K_V2\n",
    ")\n",
    "model.fc = nn.Linear(model.fc.in_features, len(test_ds.classes))\n",
    "model = model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"resnet152_layer4_finetuned.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        preds = outputs.argmax(1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_acc = (np.array(all_preds) == np.array(all_labels)).mean() * 100\n",
    "print(f\"\\nTest Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(all_labels, all_preds, target_names=test_ds.classes))\n",
    "\n",
    "cm  = confusion_matrix(all_labels, all_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=test_ds.classes)\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "disp.plot(ax=ax, xticks_rotation=45, values_format='d')\n",
    "plt.title(\"Real-test Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480db64f-fb24-4c71-8001-f61da8c4357d",
   "metadata": {},
   "source": [
    "ViT на 3 класах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23399cb6-6ea7-4a65-9aeb-37b1031f77cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms.v2 import MixUp, RandomErasing\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba6faba-1af3-4ada-90b6-1718d1a1326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),             \n",
    "    transforms.RandomResizedCrop(224),    \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandAugment(num_ops=2, magnitude=9),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.5),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),         \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad3afe3-423a-4d66-b35d-8d5d5f674a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "DATA_DIR = \"F:/GroceryStoreDataset-master/dataset\"         \n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "VAL_DIR   = os.path.join(DATA_DIR, \"val\")\n",
    "\n",
    "train_ds = ImageFolder(TRAIN_DIR, transform=train_transform)\n",
    "val_ds   = ImageFolder(VAL_DIR,   transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  num_workers=4)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "NUM_CLASSES = 3    \n",
    "\n",
    "print(f\"Train images: {len(train_ds)},  Val images: {len(val_ds)},  Classes: {NUM_CLASSES}\")\n",
    "print(\"Приклад міток перших 10 зображень:\", [train_ds[i][1] for i in range(10)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36dcdfb-804c-42e3-8cd2-021577ea6a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm, re\n",
    "\n",
    "CKPT_PATH = \"last_vit.pth\"        \n",
    "MODEL_NAME = \"vit_base_patch16_224\"\n",
    "\n",
    "model = timm.create_model(MODEL_NAME, pretrained=False, num_classes=NUM_CLASSES)\n",
    "\n",
    "state = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
    "state = {k: v for k, v in state.items()\n",
    "         if not re.match(r\"^(head|fc|classifier)\\.\", k)}   \n",
    "missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "print(f\" Backbone завантажено  |  missing = {len(missing)}, unexpected = {len(unexpected)}\")\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58401ea-5b82-4f7a-a80d-de76d9c7bc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.v2 import MixUp, RandomErasing   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a314d55e-3dd4-43ee-a258-4137a2b1446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from timm.loss import SoftTargetCrossEntropy    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "use_mixup = False          \n",
    "\n",
    "for name, p in model.named_parameters():\n",
    "    p.requires_grad = name.startswith(\"head\")\n",
    "\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                        lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "HEAD_ONLY_EPOCHS = 10\n",
    "TOTAL_EPOCHS     = 60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac5c1df-7910-4a34-8769-d385934a3c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd01f9d-de37-4292-8562-dad0f3c34213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import torch, random\n",
    "\n",
    "best_val_acc   = 0.0\n",
    "train_losses   = []\n",
    "val_accuracies = []\n",
    "patience = 6\n",
    "epochs_no_improve = 0\n",
    "\n",
    "optimizer = optim.SGD(model.head.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=HEAD_ONLY_EPOCHS)\n",
    "\n",
    "use_amp   = False          \n",
    "use_mixup = False         \n",
    "\n",
    "for epoch in range(TOTAL_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss, running_corrects, total = 0, 0, 0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{TOTAL_EPOCHS}\", leave=False)\n",
    "\n",
    "    for images, labels in loop:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        loss_fn = criterion\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if use_amp:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(images)\n",
    "                loss    = loss_fn(outputs, labels)\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss    = loss_fn(outputs, labels)\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            print(\"NaN detected — пропускаємо батч\")\n",
    "            continue\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss    += loss.item() * labels.size(0)\n",
    "        running_corrects += (outputs.argmax(1) == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        loop.set_postfix(loss=running_loss/total,\n",
    "                         acc=running_corrects/total*100)\n",
    "\n",
    "    scheduler.step()\n",
    "    train_losses.append(running_loss/total)\n",
    "\n",
    "    model.eval()\n",
    "    val_correct, val_total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            val_total   += labels.size(0)\n",
    "    val_acc = val_correct / val_total\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    if val_acc > best_val_acc + 1e-4:\n",
    "        best_val_acc = val_acc\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), \"best_vit3cls.pth\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    print(f\"Епоха {epoch+1:>2}/{TOTAL_EPOCHS} | val_acc = {val_acc*100:5.2f}% \"\n",
    "          f\"| best = {best_val_acc*100:5.2f}%\")\n",
    "\n",
    "    if epoch + 1 == HEAD_ONLY_EPOCHS:\n",
    "        model.load_state_dict(torch.load(\"best_vit3cls.pth\"))\n",
    "        print(f\"Повернули найкращий head-only чекпойнт ({best_val_acc*100:.2f} %)\")\n",
    "\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        decay, no_decay = [], []\n",
    "        for n, p in model.named_parameters():\n",
    "            if not p.requires_grad:\n",
    "                continue\n",
    "            if any(k in n.lower() for k in [\"bias\", \"norm\", \"ln\"]):\n",
    "                no_decay.append(p)\n",
    "            else:\n",
    "                decay.append(p)\n",
    "\n",
    "        optimizer = optim.AdamW(\n",
    "            [\n",
    "                {\"params\": no_decay, \"lr\": 3e-6, \"weight_decay\": 0.0},\n",
    "                {\"params\": decay,    \"lr\": 3e-6, \"weight_decay\": 1e-2},\n",
    "                {\"params\": model.head.parameters(), \"lr\": 3e-4, \"weight_decay\": 1e-2},\n",
    "            ]\n",
    "        )\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=TOTAL_EPOCHS - HEAD_ONLY_EPOCHS)\n",
    "        print(\"➜  FULL-FINETUNE PHASE (AdamW, LR_backbone = 3e-6)\")\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(\" Early-stopping (val не росте)\")\n",
    "        break\n",
    "\n",
    "log_df = pd.DataFrame({\n",
    "    \"epoch\": list(range(1, len(train_losses)+1)),\n",
    "    \"train_loss\": train_losses,\n",
    "    \"val_acc\": [x*100 for x in val_accuracies]\n",
    "})\n",
    "csv_name = f\"training_log_vit3cls_{datetime.now():%Y%m%d_%H%M%S}.csv\"\n",
    "log_df.to_csv(csv_name, index=False)\n",
    "print(f\"\\n Лог тренування збережено у «{csv_name}»\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032fd08d-a1cd-485f-a540-59b3a46a8888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "VAL_DIR = \"F:/GroceryStoreDataset-master/dataset/val\"\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_ds     = ImageFolder(VAL_DIR, transform=val_transform)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Validation samples: {len(val_ds)},  Classes: {val_ds.classes}\\n\")\n",
    "\n",
    "NUM_CLASSES = 3\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = timm.create_model(\"vit_base_patch16_224\", pretrained=False, num_classes=NUM_CLASSES)\n",
    "model.load_state_dict(torch.load(\"best_vit3cls.pth\", map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "all_preds  = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = model(images)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"Classification Report (на маленькому валідному сеті):\\n\")\n",
    "print(\n",
    "    classification_report(\n",
    "        all_labels,\n",
    "        all_preds,\n",
    "        target_names=val_ds.classes, \n",
    "        digits=2\n",
    "    )\n",
    ")\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=val_ds.classes)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "disp.plot(ax=ax, cmap=\"viridis\", xticks_rotation=45)\n",
    "\n",
    "plt.title(\"Confusion Matrix)\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097a2411-a4b8-442f-9dff-300983a46440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log_df = pd.read_csv(\"training_log_vit3cls_20250531_025153.csv\")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(log_df[\"epoch\"], log_df[\"train_loss\"], label=\"Train Loss\", color=\"steelblue\", linewidth=2)\n",
    "plt.title(\"Train Loss по епохах\", fontsize=14)\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Loss\", fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(log_df[\"epoch\"], log_df[\"val_acc\"], label=\"Val Accuracy (%)\", color=\"orange\", linewidth=2)\n",
    "plt.title(\"Validation Accuracy (%) по епохах\", fontsize=14)\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Accuracy (%)\", fontsize=12)\n",
    "plt.ylim(0, 100)      \n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29396267-744d-453b-a1ff-f6aa3b4338b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DIR = \"F:/real_test\"\n",
    "\n",
    "test_ds = ImageFolder(TEST_DIR, transform=val_transform)\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\" Test samples: {len(test_ds)}, Classes: {test_ds.classes}\")\n",
    "print(\"Приклади міток:\", [test_ds[i][1] for i in range(min(10, len(test_ds)))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949656c7-3a30-422e-927b-60f9b20d6a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),            \n",
    "    transforms.CenterCrop(224),        \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "TEST_DIR = \"F:/real_test\"   \n",
    "test_ds = ImageFolder(TEST_DIR, transform=val_transform)\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Test samples: {len(test_ds)}, Classes: {test_ds.classes}\")\n",
    "imgs, lbls = next(iter(test_loader))\n",
    "print(\"Batch image shape:\", imgs.shape, \"  Batch labels example:\", lbls[:10].tolist())\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "model = timm.create_model(\"vit_base_patch16_224\", pretrained=False, num_classes=NUM_CLASSES)\n",
    "model.load_state_dict(torch.load(\"best_vit3cls.pth\", map_location=DEVICE))\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "print(\"ViT-модель (3 класи) завантажено та у режимі eval.\")\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = model(images)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "all_preds  = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "test_acc = 100.0 * (all_preds == all_labels).mean()\n",
    "print(f\"\\n Test Accuracy: {test_acc:.2f}%\\n\")\n",
    "print(\"Classification Report:\\n\")\n",
    "print(\n",
    "    classification_report(\n",
    "        all_labels,\n",
    "        all_preds,\n",
    "        target_names=test_ds.classes,\n",
    "        digits=2\n",
    "    )\n",
    ")\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=test_ds.classes)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "disp.plot(ax=ax, cmap=\"viridis\", colorbar=True, values_format='d')\n",
    "ax.set_title(\"Confusion Matrix \", fontsize=16)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.xlabel(\"Predicted label\", fontsize=12)\n",
    "plt.ylabel(\"True label\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b5d4f1-0a6b-422e-a57a-b5f2a3edbd3e",
   "metadata": {},
   "source": [
    "convnext base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3d80af-414c-4542-914a-ad17b35ef01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import timm\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.backends.cudnn.benchmark = True  \n",
    "\n",
    "NUM_CLASSES = 3\n",
    "HEAD_ONLY_EPOCHS = 5\n",
    "FULL_EPOCHS = 5\n",
    "TOTAL_EPOCHS = HEAD_ONLY_EPOCHS + FULL_EPOCHS   \n",
    "LR_HEAD = 1e-3\n",
    "LR_FULL_BACKBONE = 3e-6\n",
    "LR_FULL_HEAD = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "PATIENCE = 3 \n",
    "\n",
    "train_transform_head = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandAugment(num_ops=2, magnitude=9),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.5),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_transform_full = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "DATA_DIR = \"F:/GroceryStoreDataset-master/dataset\"\n",
    "train_ds = ImageFolder(f\"{DATA_DIR}/train\", transform=train_transform_head)\n",
    "val_ds   = ImageFolder(f\"{DATA_DIR}/val\",   transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True,  num_workers=4)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=16, shuffle=False, num_workers=4)\n",
    "\n",
    "model = timm.create_model('convnext_base', pretrained=False, num_classes=61)\n",
    "state = torch.load(\"last_convnext.pth\", map_location=device)\n",
    "model.load_state_dict(state, strict=False)\n",
    "\n",
    "in_ch = model.head.fc.in_features\n",
    "model.head.fc = nn.Linear(in_ch, NUM_CLASSES)\n",
    "model.to(device)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in model.head.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=LR_HEAD,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=HEAD_ONLY_EPOCHS)\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler = GradScaler()\n",
    "\n",
    "log_epochs      = []\n",
    "log_train_loss  = []\n",
    "log_val_acc     = []\n",
    "\n",
    "best_val_acc = 0.0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(1, TOTAL_EPOCHS + 1):\n",
    "    if epoch == HEAD_ONLY_EPOCHS + 1:\n",
    "        train_ds.transform = train_transform_full\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        backbone_params = []\n",
    "        head_params     = []\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"head\" in name:\n",
    "                head_params.append(param)\n",
    "            else:\n",
    "                backbone_params.append(param)\n",
    "\n",
    "        optimizer = optim.AdamW([\n",
    "            {'params': backbone_params, 'lr': LR_FULL_BACKBONE},\n",
    "            {'params': head_params, 'lr': LR_FULL_HEAD}\n",
    "        ], weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=FULL_EPOCHS)\n",
    "        print(\" Розморожено всю модель, починаємо full fine-tuning\")\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    for imgs, labels in tqdm(train_loader, desc=f\"Epoch [{epoch}/{TOTAL_EPOCHS}]\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        running_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_ds)\n",
    "\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total   = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            with autocast():\n",
    "                outputs = model(imgs)\n",
    "            val_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "            val_total   += labels.size(0)\n",
    "\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    log_epochs.append(epoch)\n",
    "    log_train_loss.append(avg_train_loss)\n",
    "    log_val_acc.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{TOTAL_EPOCHS} | Train Loss = {avg_train_loss:.4f} | Val Acc = {val_acc*100:.2f}%\")\n",
    "\n",
    "    if val_acc > best_val_acc + 1e-4:\n",
    "        best_val_acc = val_acc\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), \"best_convnext_3classes.pth\")\n",
    "        print(f\"Збережено нову найкращу модель (Acc: {best_val_acc*100:.2f}%)\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"No improvement for {epochs_no_improve} epoch(s)\")\n",
    "\n",
    "    if epochs_no_improve >= PATIENCE:\n",
    "        print(f\" Early stopping: більше {PATIENCE} епох без покращення.\")\n",
    "        break\n",
    "\n",
    "print(\"Тренування завершено.\")\n",
    "\n",
    "log_df = pd.DataFrame({\n",
    "    \"epoch\":      log_epochs,\n",
    "    \"train_loss\": log_train_loss,\n",
    "    \"val_acc\":    log_val_acc\n",
    "})\n",
    "csv_filename = \"training_log_convnext3cls.csv\"\n",
    "log_df.to_csv(csv_filename, index=False)\n",
    "print(f\" Лог тренування збережено у файл: {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07daf616-0c9d-4e07-8ac7-0ec8d2e53659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import timm\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 3\n",
    "DATA_DIR = \"F:/GroceryStoreDataset-master/dataset\"\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_ds = ImageFolder(f\"{DATA_DIR}/val\", transform=val_transform)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "model = timm.create_model('convnext_base', pretrained=False, num_classes=NUM_CLASSES)\n",
    "\n",
    "checkpoint = torch.load(\"best_convnext_3classes.pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(\n",
    "    all_labels,\n",
    "    all_preds,\n",
    "    target_names=val_ds.classes,  \n",
    "    digits=4\n",
    "))\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=val_ds.classes)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "disp.plot(ax=ax, cmap=\"viridis\", xticks_rotation=45)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28628341-9a2e-48de-80af-81b42a24bcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "csv_path = \"training_log_convnext3cls.csv\"\n",
    "log_df = pd.read_csv(csv_path)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(log_df[\"epoch\"], log_df[\"train_loss\"], label=\"Train Loss\", color=\"steelblue\", linewidth=2)\n",
    "plt.title(\"Train Loss по епохах\", fontsize=14)\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Loss\", fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(log_df[\"epoch\"], log_df[\"val_acc\"]*100, label=\"Val Accuracy (%)\", color=\"orange\", linewidth=2)\n",
    "plt.title(\"Validation Accuracy (%) по епохах\", fontsize=14)\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Accuracy (%)\", fontsize=12)\n",
    "plt.ylim(0,100)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e0ed30-1299-47c2-919c-349498209915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),       \n",
    "    transforms.CenterCrop(224),   \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "TEST_DIR = \"F:/real_test\"  \n",
    "test_ds = ImageFolder(TEST_DIR, transform=val_transform)\n",
    "test_loader = DataLoader(test_ds, batch_size=16, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Test samples: {len(test_ds)}, Classes: {test_ds.classes}\")\n",
    "print(\"Приклади міток:\", [test_ds[i][1] for i in range(min(10, len(test_ds)))])\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "model = timm.create_model('convnext_base', pretrained=False, num_classes=NUM_CLASSES)\n",
    "checkpoint = torch.load(\"best_convnext_3classes.pth\", map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint)\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "print(\"ConvNeXt-Base (3 класи) завантажено та в режимі eval.\")\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        outputs = model(imgs)\n",
    "        preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "test_acc = 100.0 * (all_preds == all_labels).mean()\n",
    "print(f\"\\nTest Accuracy: {test_acc:.2f}%\\n\")\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(\n",
    "    all_labels,\n",
    "    all_preds,\n",
    "    target_names=test_ds.classes,\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=test_ds.classes)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "disp.plot(ax=ax, cmap=\"viridis\", colorbar=True, values_format='d')\n",
    "ax.set_title(\"Confusion Matrix\", fontsize=14)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.xlabel(\"Predicted label\", fontsize=12)\n",
    "plt.ylabel(\"True label\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b597fd-38bc-4713-a42a-24ca8defc138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),       \n",
    "    transforms.CenterCrop(224),   \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "TEST_DIR = \"F:/real_test\"  \n",
    "test_ds = ImageFolder(TEST_DIR, transform=val_transform)\n",
    "test_loader = DataLoader(test_ds, batch_size=16, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Test samples: {len(test_ds)}, Classes: {test_ds.classes}\")\n",
    "print(\"Приклади міток:\", [test_ds[i][1] for i in range(min(10, len(test_ds)))])\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "model = timm.create_model('convnext_base', pretrained=False, num_classes=NUM_CLASSES)\n",
    "checkpoint = torch.load(\"best_convnext_3classes.pth\", map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint)\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "print(\"ConvNeXt-Base (3 класи) завантажено та в режимі eval.\")\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        outputs = model(imgs)\n",
    "        preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "test_acc = 100.0 * (all_preds == all_labels).mean()\n",
    "print(f\"\\nTest Accuracy: {test_acc:.2f}%\\n\")\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(\n",
    "    all_labels,\n",
    "    all_preds,\n",
    "    target_names=test_ds.classes,\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=test_ds.classes)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "disp.plot(ax=ax, cmap=\"viridis\", colorbar=True, values_format='d')\n",
    "ax.set_title(\"Confusion Matrix\", fontsize=14)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.xlabel(\"Predicted label\", fontsize=12)\n",
    "plt.ylabel(\"True label\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5421388c-82d6-494a-8908-b38fb7413219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    all_labels,\n",
    "    all_preds,\n",
    "    labels=[0,1,2],            \n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "report_df = pd.DataFrame({\n",
    "    \"class_name\":   test_ds.classes,  \n",
    "    \"precision\":    precision,\n",
    "    \"recall\":       recall,\n",
    "    \"f1_score\":     f1,\n",
    "    \"support\":      support\n",
    "})\n",
    "\n",
    "report_df.loc[len(report_df)] = [\n",
    "    \"macro avg\",\n",
    "    precision.mean(),\n",
    "    recall.mean(),\n",
    "    f1.mean(),\n",
    "    support.sum()\n",
    "]\n",
    "report_df.loc[len(report_df)] = [\n",
    "    \"weighted avg\",\n",
    "    np.average(precision, weights=support),\n",
    "    np.average(recall, weights=support),\n",
    "    np.average(f1, weights=support),\n",
    "    support.sum()\n",
    "]\n",
    "\n",
    "csv_report = \"classification_report_convnext3cls.csv\"\n",
    "report_df.to_csv(csv_report, index=False)\n",
    "print(f\"Збережено Classification Report у файл: {csv_report}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533c220a-24b8-456a-ba08-4332d398c1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "csv_path = \"training_log_convnext3cls.csv\" \n",
    "log_df = pd.read_csv(csv_path)\n",
    "\n",
    "print(\"Стовпці в CSV:\", list(log_df.columns))\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(\n",
    "    log_df[\"epoch\"],\n",
    "    log_df[\"train_loss\"],\n",
    "    label=\"Train Loss\",\n",
    "    color=\"steelblue\",\n",
    "    linewidth=2\n",
    ")\n",
    "plt.title(\"Train Loss по епохах (ConvNeXt-3класи)\", fontsize=14)\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Loss\", fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(\n",
    "    log_df[\"epoch\"],\n",
    "    log_df[\"val_acc\"] * 100,  \n",
    "    label=\"Val Accuracy (%)\",\n",
    "    color=\"orange\",\n",
    "    linewidth=2\n",
    ")\n",
    "plt.title(\"Validation Accuracy (%) по епохах (ConvNeXt-3класи)\", fontsize=14)\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Accuracy (%)\", fontsize=12)\n",
    "plt.ylim(0, 100)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b69e71-c6e4-4128-b858-941de485c3ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
